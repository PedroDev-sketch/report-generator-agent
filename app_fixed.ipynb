{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae404c8e",
   "metadata": {},
   "source": [
    "# Patched local pipeline (Ollama phi3:mini)\n",
    "\n",
    "Consolidated notebook with a single pinned Ollama instance (`index_llm`), LLM helpers, local parsing, embeddings, index creation, query engine bound to `index_llm`, and a simplified report agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeedf7c-b2b8-49cd-9bd4-846cbce52c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_llm created. model attr: phi3:mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Force Ollama default to phi3:mini for any low-level calls.\n",
    "os.environ[\"OLLAMA_MODEL\"] = \"phi3:mini\"\n",
    "\n",
    "# Do this before importing the Ollama wrapper or anything that talks to Ollama:\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Create a single LlamaIndex-compatible Ollama instance and pin it:\n",
    "index_llm = Ollama(model=\"phi3:mini\", request_timeout=600.0, keep_alive=\"0s\")\n",
    "\n",
    "# Optional: set global Settings.llm to this exact instance to catch code paths that read the global.\n",
    "Settings.llm = index_llm\n",
    "\n",
    "# Diagnostics:\n",
    "print(\"index_llm created. model attr:\", getattr(index_llm, \"model\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe882ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook imports ready\n"
     ]
    }
   ],
   "source": [
    "# Imports and SSL cert setup\n",
    "import os\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "import instructor\n",
    "from instructor import Mode\n",
    "from openai import OpenAI\n",
    "\n",
    "import arxiv\n",
    "\n",
    "print('Notebook imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00ce576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM helpers created\n"
     ]
    }
   ],
   "source": [
    "# LLM helpers\n",
    "import asyncio\n",
    "\n",
    "def run_llm_sync(llm: Any, prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    if hasattr(llm, \"complete\") and callable(getattr(llm, \"complete\")):\n",
    "        resp = llm.complete(prompt)\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "    if callable(llm):\n",
    "        out = llm(prompt, max_new_tokens=max_new_tokens)\n",
    "        if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "            return out[0].get(\"generated_text\", \"\").strip()\n",
    "        if isinstance(out, dict) and \"generated_text\" in out:\n",
    "            return out[\"generated_text\"].strip()\n",
    "        return str(out).strip()\n",
    "    try:\n",
    "        from openai import OpenAI as _OpenAI\n",
    "        if isinstance(llm, _OpenAI):\n",
    "            resp = llm.chat.completions.create(model=\"phi3:mini\", messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=max_new_tokens)\n",
    "            return resp.choices[0].message.content.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        out = llm(prompt)\n",
    "        return str(out).strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM sync call failed: {e}\")\n",
    "\n",
    "async def run_llm_async(llm: Any, prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    if hasattr(llm, \"acomplete\") and callable(getattr(llm, \"acomplete\")):\n",
    "        resp = await llm.acomplete(prompt)\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, run_llm_sync, llm, prompt, max_new_tokens)\n",
    "\n",
    "print('LLM helpers created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d23ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_llm and metadata_client created\n",
      "index_llm.model = phi3:mini\n"
     ]
    }
   ],
   "source": [
    "# Create and pin one Ollama instance for the index\n",
    "index_llm = Ollama(model=\"phi3:mini\", request_timeout=600.0, keep_alive=\"0s\", additional_kwargs={\"num_ctx\": 4096})\n",
    "Settings.llm = index_llm\n",
    "\n",
    "# Instructor/OpenAI client for metadata extraction (separate)\n",
    "metadata_client = instructor.patch(\n",
    "    OpenAI(base_url='http://localhost:11434/v1', api_key='ollama'),\n",
    "    mode=Mode.JSON,\n",
    ")\n",
    "print('index_llm and metadata_client created')\n",
    "print('index_llm.model =', getattr(index_llm, 'model', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2c3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 2510.11483v1.Uncertainty_Quantification_for_Retrieval_Augmented_Reasoning.pdf\n",
      "Parsed: 2510.11541v1.Query_Specific_GNN__A_Comprehensive_Graph_Representation_Learning_Method_for_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.11654v1.FinVet__A_Collaborative_Framework_of_RAG_and_External_Fact_Checking_Agents_for_Financial_Misinformation_Detection.pdf\n",
      "Parsed: 2510.11694v1.Operand_Quant__A_Single_Agent_Architecture_for_Autonomous_Machine_Learning_Engineering.pdf\n",
      "Parsed: 2510.11695v1.When_Agents_Trade__Live_Multi_Market_Trading_Benchmark_for_LLM_Agents.pdf\n",
      "Parsed: 2510.11701v1.Demystifying_Reinforcement_Learning_in_Agentic_Reasoning.pdf\n",
      "Parsed: 2510.12460v1.Probing_Latent_Knowledge_Conflict_for_Faithful_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.12668v1.The_Role_of_Parametric_Injection_A_Systematic_Study_of_Parametric_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.12750v1.VQArt_Bench__A_semantically_rich_VQA_Benchmark_for_Art_and_Cultural_Heritage.pdf\n",
      "Parsed: 2510.12787v1.Ax_Prover__A_Deep_Reasoning_Agentic_Framework_for_Theorem_Proving_in_Mathematics_and_Quantum_Physics.pdf\n",
      "Parsed: 2510.12801v1.DeepMMSearch_R1__Empowering_Multimodal_LLMs_in_Multimodal_Web_Search.pdf\n",
      "Parsed documents: 184\n"
     ]
    }
   ],
   "source": [
    "# Parse local PDFs (if any)\n",
    "from pathlib import Path\n",
    "reader = PDFReader()\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    return [str(p) for p in Path(directory).glob('*.pdf')]\n",
    "\n",
    "def parse_files(pdf_files):\n",
    "    all_documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            docs = reader.load_data(pdf_file)\n",
    "            if isinstance(docs, tuple):\n",
    "                docs = list(docs)\n",
    "            elif not isinstance(docs, list):\n",
    "                docs = [docs]\n",
    "            all_documents.extend(docs)\n",
    "            print('Parsed:', pdf_file)\n",
    "        except Exception as e:\n",
    "            print('Failed to parse', pdf_file, e)\n",
    "    return all_documents\n",
    "\n",
    "pdf_files = list_pdf_files('.')\n",
    "documents = parse_files(pdf_files)\n",
    "print('Parsed documents:', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca59042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 04:28:13,319 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-10-15 04:28:17,442 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-10-15 04:30:27,516 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index and query engine ready\n"
     ]
    }
   ],
   "source": [
    "# Embeddings, chunking, index and query engine\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "transform = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "all_nodes = []\n",
    "for doc in documents:\n",
    "    all_nodes.extend(transform.get_nodes_from_documents([doc]))\n",
    "\n",
    "index = VectorStoreIndex.from_documents(all_nodes, embed_model=embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode='compact',\n",
    "    llm=index_llm,\n",
    "    verbose=True,\n",
    ")\n",
    "print('Index and query engine ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb455dbf-2e76-41f6-9df7-7e0141dc772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama ps (server-side):\n",
      "NAME    ID    SIZE    PROCESSOR    CONTEXT    UNTIL \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 04:30:28,324 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oc.show('phi3:mini') OK. modelinfo keys: ['general.architecture', 'general.basename', 'general.file_type', 'general.finetune', 'general.languages', 'general.license', 'general.license.link', 'general.parameter_count', 'general.quantization_version', 'general.size_label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 04:31:01,188 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_llm.complete.text: Hello, I'm Phi3 and this is my mini introduction. As a virtual assistant designed to interact with users like you on platforms such as Club Penguin, remember that while I am here for playful conversations and immersive experiences in the game world of penguins or any similar environment, itâ€™s import\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 04:33:37,376 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_engine success. Short repr: RAG refers to a technique where external documents are retrieved during inference for supplementation of language models' internal knowledge in order to enhance performance on tasks that require factual information, while also dealing with limitations such as increased context length and potential comprehension issues.\n"
     ]
    }
   ],
   "source": [
    "#Diagnostic Cell\n",
    "\n",
    "import traceback\n",
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# Show which models are running on the server\n",
    "print(\"ollama ps (server-side):\")\n",
    "import subprocess, json, sys\n",
    "try:\n",
    "    out = subprocess.check_output([\"ollama\", \"ps\"], stderr=subprocess.STDOUT)\n",
    "    print(out.decode())\n",
    "except Exception as e:\n",
    "    print(\"Could not run ollama ps from notebook:\", e)\n",
    "\n",
    "# Low-level check of model info:\n",
    "oc = OllamaClient()\n",
    "try:\n",
    "    info = oc.show(\"phi3:mini\")\n",
    "    print(\"oc.show('phi3:mini') OK. modelinfo keys:\", list(info.modelinfo.keys())[:10])\n",
    "except Exception as e:\n",
    "    print(\"oc.show failed:\", e)\n",
    "\n",
    "try:\n",
    "    resp = index_llm.complete(\"Say hello from phi3:mini\")\n",
    "    if hasattr(resp, \"text\"):\n",
    "        print(\"index_llm.complete.text:\", resp.text[:300])\n",
    "    else:\n",
    "        print(\"index_llm.complete raw repr:\", repr(resp)[:400])\n",
    "except Exception as e:\n",
    "    print(\"index_llm.complete raised:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "try:\n",
    "    qres = query_engine.query(\"What is Retrieval-Augmented Generation?\")\n",
    "    print(\"query_engine success. Short repr:\", str(qres)[:400])\n",
    "except Exception as e:\n",
    "    print(\"query_engine raised:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6912ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions ready\n"
     ]
    }
   ],
   "source": [
    "# Helper functions used by the agent\n",
    "import re\n",
    "\n",
    "def extract_title(outline):\n",
    "    first_line = outline.strip().split('\\n')[0]\n",
    "    return first_line.strip('# ').strip()\n",
    "\n",
    "# reuse parse_outline_and_generate_queries from earlier patterns\n",
    "def parse_outline_and_generate_queries(outline, llm):\n",
    "    if isinstance(outline, str):\n",
    "        lines = outline.splitlines()\n",
    "        title = 'Untitled Report'\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line = line.strip().lstrip('#').strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if re.match(r\"^\\d+\\.\", line) and not re.match(r\"^\\d+\\.\\d+\\.\", line):\n",
    "                current_section = line\n",
    "                sections[current_section] = []\n",
    "            elif re.match(r\"^\\d+\\.\\d+\\.\", line) and current_section:\n",
    "                sections[current_section].append(line)\n",
    "        outline = {\"title\": title, \"sections\": sections}\n",
    "    title = outline.get('title', 'Untitled Report')\n",
    "    sections = outline.get('sections', {})\n",
    "    queries = {}\n",
    "    for section, subsections in sections.items():\n",
    "        queries[section] = {}\n",
    "        for subsection in subsections:\n",
    "            prompt = f\"Generate one clear, concise query for the subsection '{subsection}' under section '{section}' in the report titled '{title}'.\"\n",
    "            query_text = run_llm_sync(llm, prompt)\n",
    "            # simple classify: shorter -> LLM else INDEX (you may replace with a real classifier)\n",
    "            classification = 'LLM' if len(query_text.split()) < 40 else 'INDEX'\n",
    "            queries[section][subsection] = {'query': query_text, 'classification': classification}\n",
    "    return queries\n",
    "\n",
    "print('Helper functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599bcbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ready\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, Context, step\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "# Define the event used to pass queries from one step to the next\n",
    "class ReportGenerationEvent(Event):\n",
    "    \"\"\"Event carrying generated queries for report generation.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(self, query_engine: Any, llm: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.query_engine = query_engine\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate_section_content(self, queries: Dict[str, Dict[str, Dict[str, str]]], reverse: bool = False) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Generate content for each section/subsection. Returns a mapping:\n",
    "        { section_title: { subsection_key: answer_text, ... }, ... }\n",
    "        \"\"\"\n",
    "        section_contents: Dict[str, Dict[str, str]] = {}\n",
    "        for section, subsections in queries.items():\n",
    "            section_contents[section] = {}\n",
    "            subsection_keys = reversed(sorted(subsections.keys())) if reverse else sorted(subsections.keys())\n",
    "            for subsection in subsection_keys:\n",
    "                data = subsections[subsection]\n",
    "                query = data['query']\n",
    "                classification = data['classification']\n",
    "                if classification == \"LLM\":\n",
    "                    answer = run_llm_sync(self.llm, query + \" Give a short answer.\")\n",
    "                else:\n",
    "                    # Use the query engine for INDEX queries\n",
    "                    answer = str(self.query_engine.query(query))\n",
    "                section_contents[section][subsection] = answer\n",
    "        return section_contents\n",
    "\n",
    "    async def format_report(self, section_contents: Dict[str, Dict[str, str]], outline: Any) -> str:\n",
    "        \"\"\"Format the report from the generated section contents and outline; returns markdown string.\"\"\"\n",
    "        report = \"\"\n",
    "\n",
    "        for section, subsections in section_contents.items():\n",
    "            section_match = re.match(r'^(\\d+\\.)\\s*(.*)$', section)\n",
    "            if section_match:\n",
    "                section_num, section_title = section_match.groups()\n",
    "\n",
    "                if \"introduction\" in section.lower():\n",
    "                    introduction_num, introduction_title = section_num, section_title\n",
    "                elif \"conclusion\" in section.lower():\n",
    "                    conclusion_num, conclusion_title = section_num, section_title\n",
    "                else:\n",
    "                    combined_content = \"\\n\".join(subsections.values())\n",
    "                    summary_query = f\"Provide a short summary for section '{section}':\\n\\n{combined_content}\"\n",
    "                    section_summary = run_llm_sync(self.llm, summary_query)\n",
    "\n",
    "                    report += f\"# {section_num} {section_title}\\n\\n{section_summary}\\n\\n\"\n",
    "                    report = self.get_subsections_content(subsections, report)\n",
    "\n",
    "        # Add introduction (async)\n",
    "        introduction_query = f\"Create an introduction for the report:\\n\\n{report}\"\n",
    "        introduction = await run_llm_async(self.llm, introduction_query)\n",
    "        # Prepend introduction\n",
    "        report = f\"# {introduction_num} {introduction_title}\\n\\n{introduction}\\n\\n\" + report\n",
    "\n",
    "        # Add conclusion (async)\n",
    "        conclusion_query = f\"Create a conclusion for the report:\\n\\n{report}\"\n",
    "        conclusion = await run_llm_async(self.llm, conclusion_query)\n",
    "        report += f\"# {conclusion_num} {conclusion_title}\\n\\n{conclusion}\"\n",
    "\n",
    "        # Add title\n",
    "        title = extract_title(outline) if isinstance(outline, str) else outline.get(\"title\", \"Report\")\n",
    "        report = f\"# {title}\\n\\n{report}\"\n",
    "        return report\n",
    "\n",
    "    def get_subsections_content(self, subsections: Dict[str, str], report: str) -> str:\n",
    "        \"\"\"Generate content for each subsection in the outline and append to report string.\"\"\"\n",
    "        for subsection in sorted(subsections.keys(), key=lambda x: (re.search(r'(\\d+\\.\\d+)', x).group(1) if re.search(r'(\\d+\\.\\d+)', x) else x)):\n",
    "            content = subsections[subsection]\n",
    "            subsection_match = re.search(r'(\\d+\\.\\d+)\\.\\s*(.+)', subsection)\n",
    "            if subsection_match:\n",
    "                subsection_num, subsection_title = subsection_match.groups()\n",
    "                report += f\"## {subsection_num} {subsection_title}\\n\\n{content}\\n\\n\"\n",
    "            else:\n",
    "                report += f\"## {subsection}\\n\\n{content}\\n\\n\"\n",
    "        return report\n",
    "\n",
    "    # ---- workflow steps: MUST have explicit return type annotations ----\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def queries_generation_event(self, ctx: Context, ev: StartEvent) -> ReportGenerationEvent:\n",
    "        \"\"\"Generate queries for the report and emit ReportGenerationEvent.\"\"\"\n",
    "        # store outline on context for downstream steps\n",
    "        ctx.outline = ev.outline\n",
    "        outline = ctx.outline\n",
    "        queries = parse_outline_and_generate_queries(outline, self.llm)\n",
    "        return ReportGenerationEvent(queries=queries)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(self, ctx: Context, ev: ReportGenerationEvent) -> StopEvent:\n",
    "        \"\"\"Generate the report and return it in a StopEvent result.\"\"\"\n",
    "        outline = ctx.outline\n",
    "        queries = ev.queries\n",
    "\n",
    "        section_contents = self.generate_section_content(queries, reverse=True)\n",
    "        report = await self.format_report(section_contents, outline)\n",
    "\n",
    "        return StopEvent(result={\"response\": report})\n",
    "\n",
    "print('Agent ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed47d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query engine smoke test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 04:36:45,749 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - query engine response preview: Retrieval-augmented generation retrieves external documents to supplement large language model's internal knowledge for enhancing performance on tasks requiring factual information and mitigating hallucinations. It aims at addressing limitations of traditional methods by incorporating additional relevant data sources during the inference time, facilitating better task execution in domains where co\n",
      "Agent run failed: asyncio.run() cannot be called from a running event loop\n"
     ]
    }
   ],
   "source": [
    "# Outline and run (test). Note: running the agent can be slow depending on model speed.\n",
    "outline = '''\n",
    "# Research Paper Report on RAG - Retrieval Augmented Generation and Agentic World.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Retrieval Augmented Generation (RAG) and Agents\n",
    "2.1. Fundamentals of RAG and Agents.\n",
    "2.2. Current State and Applications\n",
    "\n",
    "## 3. Latest Papers:\n",
    "3.1. HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings\n",
    "3.2. MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems\n",
    "3.3. VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding\n",
    "\n",
    "## 4. Conclusion:\n",
    "'''\n",
    "\n",
    "# Quick engine smoke test\n",
    "print('Query engine smoke test:')\n",
    "try:\n",
    "    res = query_engine.query('What is Retrieval-Augmented Generation?')\n",
    "    print('OK - query engine response preview:', str(res)[:400])\n",
    "except Exception as e:\n",
    "    print('Query engine failed:', e)\n",
    "\n",
    "# Run agent\n",
    "agent = ReportGenerationAgent(query_engine=query_engine, llm=index_llm)\n",
    "import asyncio\n",
    "try:\n",
    "    rep = asyncio.run(agent.run(outline=outline))\n",
    "    print('Agent completed. Writing report.md')\n",
    "    with open('report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(rep['response'])\n",
    "    print('Wrote report.md')\n",
    "except Exception as e:\n",
    "    print('Agent run failed:', e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
