{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd9175c-66cb-4371-8275-a87ab7d265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from instructor import Mode\n",
    "import nest_asyncio\n",
    "from openai import OpenAI\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6ec897-a37a-464d-972a-9834f424f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "\n",
    "#ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#needs to be changed for production - it's currently unprotected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f266ae-ad40-4656-b1d0-95a67a5d8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any\n",
    "\n",
    "def run_llm_sync(llm: Any, prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper that handles:\n",
    "      - Ollama wrapper with .complete()\n",
    "      - HF pipeline-like callables that return list/dict with 'generated_text'\n",
    "      - Generic callable returning a string\n",
    "      - OpenAI client objects\n",
    "    \"\"\"\n",
    "    # NEW: Handle OpenAI client (including instructor-patched ones)\n",
    "    if isinstance(llm, OpenAI):\n",
    "        response = llm.chat.completions.create(\n",
    "            model=\"phi3:mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_new_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    # Ollama synchronous wrapper (llama_index.llms.ollama.Ollama provides .complete())\n",
    "    if hasattr(llm, \"complete\") and callable(getattr(llm, \"complete\")):\n",
    "        resp = llm.complete(prompt)\n",
    "        # response may have .text\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "\n",
    "    # If llm is a huggingface pipeline (callable)\n",
    "    if callable(llm):\n",
    "        out = llm(prompt, max_new_tokens=max_new_tokens)\n",
    "        # HF pipeline returns list[{\"generated_text\": \"...\"}]\n",
    "        if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "            return out[0].get(\"generated_text\", \"\").strip()\n",
    "        # Some pipelines return a dict\n",
    "        if isinstance(out, dict) and \"generated_text\" in out:\n",
    "            return out[\"generated_text\"].strip()\n",
    "        # fallback\n",
    "        return str(out).strip()\n",
    "\n",
    "    # Last resort: try calling and stringifying\n",
    "    try:\n",
    "        out = llm(prompt)\n",
    "        return str(out).strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM sync call failed: {e}\")\n",
    "\n",
    "async def run_llm_async(llm: Any, prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Async wrapper that tries to use llm.acomplete() if present, otherwise\n",
    "    runs the synchronous wrapper in a thread pool.\n",
    "    \"\"\"\n",
    "    # If the wrapper supports async completion (ollama .acomplete)\n",
    "    if hasattr(llm, \"acomplete\") and callable(getattr(llm, \"acomplete\")):\n",
    "        resp = await llm.acomplete(prompt)\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "\n",
    "    # Fallback: run the synchronous call in executor\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, run_llm_sync, llm, prompt, max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53a93814-5705-4713-8a61-c17d1314353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "Settings.llm = Ollama(model=\"phi3:mini\", request_timeout=600.0, stream=True, keep_alive=\"0s\", additional_kwargs={\"num_ctx\": 4096})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6fadc-600a-4c9d-8b9c-a210dcee6790",
   "metadata": {},
   "source": [
    "## Download arxiv papers based on topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e45ad4c3-9694-43db-b885-ee0fcde1ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_topics = [\"RAG\", \"Agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbda7d8b-63e4-4172-a89c-d2334dc21ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def download_papers(client, topics, num_results_per_topic):\n",
    "    \"\"\"Function to download papers from arxiv for given topics and number of results per topic\"\"\"\n",
    "    for topic in topics:\n",
    "\n",
    "        # sort by recent data and with max results\n",
    "        search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = num_results_per_topic,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        # get the results\n",
    "        results = client.results(search)\n",
    "\n",
    "        # download the pdf\n",
    "        for r in results:\n",
    "            r.download_pdf()\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    # List all .pdf files using pathlib\n",
    "    pdf_files = [file.name for file in Path(directory).glob('*.pdf')]\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21ca0794-2ecc-4255-b1d6-588ea2145c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:30:40,205 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=RAG&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n",
      "2025-10-15 11:30:42,574 - INFO - Got first page: 100 of 3242 total results\n",
      "2025-10-15 11:30:44,724 - INFO - Sleeping: 0.519159 seconds\n",
      "2025-10-15 11:30:45,318 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Agent&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n",
      "2025-10-15 11:30:46,981 - INFO - Got first page: 100 of 53085 total results\n"
     ]
    }
   ],
   "source": [
    "# create a client\n",
    "client = arxiv.Client()\n",
    "\n",
    "download_papers(client, research_paper_topics, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89c1a8-0892-482d-9bc3-fba9a3f9249e",
   "metadata": {},
   "source": [
    "## Parsing the documents using LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc1f166-7b06-4319-9219-14fa2c4b402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "def parse_files(pdf_files):\n",
    "    \"\"\"Parse PDFs locally (no API key required) and return a flat list of Document objects.\"\"\"\n",
    "    reader = PDFReader()\n",
    "    all_documents = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            docs = reader.load_data(pdf_file)\n",
    "            \n",
    "            # Flatten in case it's a list of tuples or nested lists\n",
    "            if isinstance(docs, tuple):\n",
    "                docs = list(docs)\n",
    "            elif not isinstance(docs, list):\n",
    "                docs = [docs]\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "            print(f\"Parsed: {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {pdf_file}: {e}\")\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9044a7f9-0e37-4338-abd0-892594735276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 2510.11483v1.Uncertainty_Quantification_for_Retrieval_Augmented_Reasoning.pdf\n",
      "Parsed: 2510.11541v1.Query_Specific_GNN__A_Comprehensive_Graph_Representation_Learning_Method_for_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.11654v1.FinVet__A_Collaborative_Framework_of_RAG_and_External_Fact_Checking_Agents_for_Financial_Misinformation_Detection.pdf\n",
      "Parsed: 2510.11694v1.Operand_Quant__A_Single_Agent_Architecture_for_Autonomous_Machine_Learning_Engineering.pdf\n",
      "Parsed: 2510.11695v1.When_Agents_Trade__Live_Multi_Market_Trading_Benchmark_for_LLM_Agents.pdf\n",
      "Parsed: 2510.11701v1.Demystifying_Reinforcement_Learning_in_Agentic_Reasoning.pdf\n",
      "Parsed: 2510.12460v1.Probing_Latent_Knowledge_Conflict_for_Faithful_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.12668v1.The_Role_of_Parametric_Injection_A_Systematic_Study_of_Parametric_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.12750v1.VQArt_Bench__A_semantically_rich_VQA_Benchmark_for_Art_and_Cultural_Heritage.pdf\n",
      "Parsed: 2510.12787v1.Ax_Prover__A_Deep_Reasoning_Agentic_Framework_for_Theorem_Proving_in_Mathematics_and_Quantum_Physics.pdf\n",
      "Parsed: 2510.12801v1.DeepMMSearch_R1__Empowering_Multimodal_LLMs_in_Multimodal_Web_Search.pdf\n"
     ]
    }
   ],
   "source": [
    "directory = './'\n",
    "pdf_files = list_pdf_files(directory)\n",
    "\n",
    "documents = parse_files(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ef05-f1fa-4dea-be6e-21909b7da107",
   "metadata": {},
   "source": [
    "## LlamaIndex Local Logic that substitutes the LlamaCloud Pipeline\n",
    "in an effort to circumvent subscription fees to paid embedding services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab9ee79-55c1-42e4-9f9d-f005a9cd070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:31:29,331 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-10-15 11:31:33,800 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-10-15 11:36:33,039 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Local embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Local chunking configuration\n",
    "transform = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# Flatten parsed LlamaParse outputs into text nodes\n",
    "all_nodes = []\n",
    "for doc in documents:\n",
    "    all_nodes.extend(transform.get_nodes_from_documents([doc]))\n",
    "\n",
    "# Create local vector index\n",
    "index = VectorStoreIndex.from_documents(all_nodes, embed_model=embed_model)\n",
    "\n",
    "# Create a query engine from this index\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ba9e8-ca9d-4a69-804d-4fdd037e8728",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Here, we define some utilities to help us extract metadata from each document.\n",
    "\n",
    "* Metadata - Pydantic model to extract metadata of author names, companies and general AI tags.\n",
    "* get_papers_metadata - Extracts the metadata information from the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "271f390e-5b8c-4a3e-bf63-8618618912c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Output containing the authors names, authors companies, and general AI tags.\"\"\"\n",
    "\n",
    "    author_names: List[str] = Field(..., description=\"List of author names of the paper. Give empty list if not available\")\n",
    "\n",
    "    author_companies: List[str] = Field(..., description=\"List of author companies of the paper. Give empty list if not available\")\n",
    "\n",
    "    ai_tags: List[str] = Field(..., description=\"List of general AI tags related to the paper. Give empty list if not available\")\n",
    "\n",
    "client = instructor.patch(\n",
    "    OpenAI(\n",
    "        base_url='http://localhost:11434/v1',\n",
    "        api_key='ollama',  # required but unused\n",
    "    ),\n",
    "    mode=Mode.JSON,\n",
    ")\n",
    "\n",
    "async def get_papers_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given paper\"\"\"\n",
    "    prompt = f\"\"\"Generate authors names, authors companies, and general top 3 AI tags for the given research paper.\n",
    "\n",
    "Research Paper:\n",
    "{text}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"phi3:mini\",\n",
    "        response_model=Metadata,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Then just use client directly for other LLM calls\n",
    "llm = client  # Simple assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df1a4e3-7ce3-4fc0-8611-6cdfae359502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_title(outline):\n",
    "    \"\"\"Function to extract the title from the first line of the outline\"\"\"\n",
    "\n",
    "    first_line = outline.strip().split('\\n')[0]\n",
    "    return first_line.strip('# ').strip()\n",
    "\n",
    "def generate_query_with_llm_local(title, section, subsection, llm):\n",
    "    \"\"\"Generate a concise research query using a local LLM pipeline.\"\"\"\n",
    "    prompt = (\n",
    "        f\"Generate a concise research query for the report titled '{title}'. \"\n",
    "        f\"The query should be for the subsection '{subsection}' under the section '{section}'. \"\n",
    "        f\"The query should help gather relevant information for this part of the report.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Works for local pipelines (e.g. HuggingFace Transformers)\n",
    "        output = llm(prompt, max_new_tokens=100)\n",
    "        \n",
    "        # Handle both dict (transformers) and string outputs\n",
    "        if isinstance(output, list) and \"generated_text\" in output[0]:\n",
    "            return output[0][\"generated_text\"].strip()\n",
    "        elif isinstance(output, dict) and \"generated_text\" in output:\n",
    "            return output[\"generated_text\"].strip()\n",
    "        elif isinstance(output, str):\n",
    "            return output.strip()\n",
    "        else:\n",
    "            return str(output).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate query for {section} → {subsection}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def classify_query(query):\n",
    "    \"\"\"Function to classify the query as either 'LLM' or 'INDEX' based on the query content\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Classify the following query as either \"LLM\" if it can be answered directly by a large language model with general knowledge, or \"INDEX\" if it likely requires querying an external index or database for specific or up-to-date information.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Consider the following:\n",
    "    1. If the query asks for general knowledge, concepts, or explanations, classify as \"LLM\".\n",
    "    2. If the query asks for specific facts, recent events, or detailed information that might not be in the LLM's training data, classify as \"INDEX\".\n",
    "    3. If unsure, err on the side of \"INDEX\".\n",
    "\n",
    "    Classification:\"\"\"\n",
    "\n",
    "    classification = run_llm_sync(llm, prompt).upper()\n",
    "\n",
    "    if classification not in [\"LLM\", \"INDEX\"]:\n",
    "        classification = \"INDEX\"  # Default to INDEX if the response is unclear\n",
    "\n",
    "    return classification\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_outline_and_generate_queries(outline, llm):\n",
    "    \"\"\"\n",
    "    Parse the outline (string or dict) and use a *local* LLM pipeline\n",
    "    to generate one query per subsection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 — Convert outline string → dict automatically\n",
    "    if isinstance(outline, str):\n",
    "        lines = outline.splitlines()\n",
    "        title = \"Untitled Report\"\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip().lstrip('#').strip()\n",
    "            if not line:\n",
    "                continue  # skip blank lines\n",
    "\n",
    "            # Example: \"1. Introduction\"\n",
    "            if re.match(r\"^\\d+\\.\", line) and not re.match(r\"^\\d+\\.\\d+\\.\", line):\n",
    "                current_section = line\n",
    "                sections[current_section] = []\n",
    "\n",
    "            # Example: \"1.1 Background\"\n",
    "            elif re.match(r\"^\\d+\\.\\d+\\.\", line) and current_section:\n",
    "                sections[current_section].append(line)\n",
    "\n",
    "        outline = {\"title\": title, \"sections\": sections}\n",
    "\n",
    "    # Step 2 — Safely access fields\n",
    "    title = outline.get(\"title\", \"Untitled Report\")\n",
    "    sections = outline.get(\"sections\", {})\n",
    "\n",
    "    queries = {}\n",
    "\n",
    "    # Step 3 — Generate a query per subsection using local llm\n",
    "    for section, subsections in sections.items():\n",
    "        queries[section] = {}\n",
    "        for subsection in subsections:\n",
    "            prompt = (\n",
    "                f\"Generate one clear, concise query for the subsection '{subsection}' \"\n",
    "                f\"under section '{section}' in the report titled '{title}'. \"\n",
    "                f\"The query should guide research to gather relevant information.\"\n",
    "            )\n",
    "\n",
    "            # Run the LLM locally (synchronous or pipeline-based)\n",
    "            # Run the LLM using the sync helper function\n",
    "            # 1. Generate the query using the sync helper\n",
    "            query_text = run_llm_sync(llm, prompt)\n",
    "            \n",
    "            # 2. Classify the generated query\n",
    "            classification = classify_query(query_text)\n",
    "            \n",
    "            # 3. Store both the query and its classification in a dictionary\n",
    "            queries[section][subsection] = {\n",
    "                'query': query_text,\n",
    "                'classification': classification\n",
    "            }\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967dc4d-6bc7-4c1b-b542-b2dc981b570d",
   "metadata": {},
   "source": [
    "## ReportGenerationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd8252a-0d0a-40cd-8ff7-c161e4971dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, Context, step\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ReportGenerationEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_engine: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.query_engine = query_engine\n",
    "        self.llm = llm or OpenAI(model='gpt-4o-mini')\n",
    "\n",
    "    async def format_report(self, section_contents, outline):\n",
    "        \"\"\"Format the report based on the section contents.\"\"\"\n",
    "        report = \"\"\n",
    "\n",
    "        for section, subsections in section_contents.items():\n",
    "            section_match = re.match(r'^(\\d+\\.)\\s*(.*)$', section)\n",
    "            if section_match:\n",
    "                section_num, section_title = section_match.groups()\n",
    "                \n",
    "                if \"introduction\" in section.lower():\n",
    "                    introduction_num, introduction_title = section_num, section_title\n",
    "                elif \"conclusion\" in section.lower():\n",
    "                    conclusion_num, conclusion_title = section_num, section_title\n",
    "                else:\n",
    "                    combined_content = \"\\n\".join(subsections.values())\n",
    "                    summary_query = f\"Provide a short summary for section '{section}':\\n\\n{combined_content}\"\n",
    "                    section_summary = run_llm_sync(self.llm, summary_query)\n",
    "\n",
    "                    report += f\"# {section_num} {section_title}\\n\\n{section_summary}\\n\\n\"\n",
    "\n",
    "                    report = self.get_subsections_content(subsections, report)\n",
    "\n",
    "        # Add introduction\n",
    "\n",
    "        introduction_query = f\"Create an introduction for the report:\\n\\n{report}\"\n",
    "        introduction = await run_llm_async(self.llm, introduction_query)\n",
    "        report = f\"# {introduction_num} {introduction_title}\\n\\n{introduction}\\n\\n\" + report\n",
    "\n",
    "        # Add conclusion\n",
    "\n",
    "        conclusion_query = f\"Create a conclusion for the report:\\n\\n{report}\"\n",
    "        conclusion = await run_llm_async(self.llm, conclusion_query)\n",
    "        report += f\"# {conclusion_num} {conclusion_title}\\n\\n{conclusion}\"\n",
    "\n",
    "        # Add title\n",
    "        title = extract_title(outline)\n",
    "        report = f\"# {title}\\n\\n{report}\"\n",
    "        return report\n",
    "\n",
    "    def get_subsections_content(self, subsections, report):\n",
    "        \"\"\"Generate content for each subsection in the outline.\"\"\"\n",
    "        # Sort subsections by their keys before adding them to the report\n",
    "        for subsection in sorted(subsections.keys(), key=lambda x: re.search(r'(\\d+\\.\\d+)', x).group(1) if re.search(r'(\\d+\\.\\d+)', x) else x):\n",
    "            content = subsections[subsection]\n",
    "            subsection_match = re.search(r'(\\d+\\.\\d+)\\.\\s*(.+)', subsection)\n",
    "            if subsection_match:\n",
    "                subsection_num, subsection_title = subsection_match.groups()\n",
    "                report += f\"## {subsection_num} {subsection_title}\\n\\n{content}\\n\\n\"\n",
    "            else:\n",
    "                report += f\"## {subsection}\\n\\n{content}\\n\\n\"\n",
    "        return report\n",
    "\n",
    "    def generate_section_content(self, queries, reverse=False):\n",
    "        \"\"\"Generate content for each section and subsection in the outline.\"\"\"\n",
    "        section_contents = {}\n",
    "        for section, subsections in queries.items():\n",
    "            section_contents[section] = {}\n",
    "            subsection_keys = reversed(sorted(subsections.keys())) if reverse else sorted(subsections.keys())\n",
    "            for subsection in subsection_keys:\n",
    "                data = subsections[subsection]\n",
    "                query = data['query']\n",
    "                classification = data['classification']\n",
    "                if classification == \"LLM\":\n",
    "                    # if inside class method and self.llm exists:\n",
    "                    answer = run_llm_sync(self.llm, query + \" Give a short answer.\")\n",
    "\n",
    "                else:\n",
    "                    answer = str(query_engine.query(query))\n",
    "                section_contents[section][subsection] = answer\n",
    "        return section_contents\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def queries_generation_event(self, ctx: Context, ev: StartEvent) -> ReportGenerationEvent:\n",
    "        \"\"\"Generate queries for the report.\"\"\"\n",
    "        \n",
    "        # Store outline directly as an attribute on ctx\n",
    "        ctx.outline = ev.outline\n",
    "    \n",
    "        outline = ctx.outline\n",
    "        queries = parse_outline_and_generate_queries(outline, self.llm)\n",
    "    \n",
    "        return ReportGenerationEvent(queries=queries)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(self, ctx: Context, ev: ReportGenerationEvent) -> StopEvent:\n",
    "        \"\"\"Generate report.\"\"\"\n",
    "        \n",
    "        outline = ctx.outline\n",
    "        queries = ev.queries\n",
    "    \n",
    "        section_contents = self.generate_section_content(queries, reverse=True)\n",
    "        report = await self.format_report(section_contents, outline)\n",
    "    \n",
    "        return StopEvent(result={\"response\": report})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36933c-b76d-456d-995d-5fd2bc339d33",
   "metadata": {},
   "source": [
    "## Outline of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9df5af4-3f29-4400-8ac1-6e0de44988e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = \"\"\"\n",
    "# Research Paper Report on RAG - Retrieval Augmented Generation and Agentic World.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Retrieval Augmented Generation (RAG) and Agents\n",
    "2.1. Fundamentals of RAG and Agents.\n",
    "2.2. Current State and Applications\n",
    "\n",
    "## 3. Latest Papers:\n",
    "3.1. HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings\n",
    "3.2. MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems\n",
    "3.3. VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding\n",
    "\n",
    "## 4. Conclusion:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca6a4e14-cbaf-40ab-9385-b754c18d1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the query engine with a simple query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:45:12,274 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! The query engine is working correctly.\n",
      "Response: Retrieval-Augmented Generation (RAG) enhances Large Language Models by retrieving relevant external documents or information sources to generate more informed and contextually accurate responses, thus reducing hallucinations in language models. It integrates retrieved knowledge into the generation process for grounding responses with factual accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Add this in a new cell after the query_engine is created\n",
    "print(\"Testing the query engine with a simple query...\")\n",
    "try:\n",
    "    response = query_engine.query(\"What is Retrieval-Augmented Generation?\")\n",
    "    print(\"SUCCESS! The query engine is working correctly.\")\n",
    "    print(\"Response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: The query engine failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc625db0-4bca-4292-82ea-8bb7f6c6bd34",
   "metadata": {},
   "source": [
    "## Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44b945b5-ac1e-4ee7-a8c3-93727558a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step queries_generation_event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 11:45:32,880 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:46:11,384 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:46:53,190 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:47:41,402 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:48:32,584 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:49:37,947 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:49:51,160 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:50:20,120 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:50:55,148 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-15 11:52:10,083 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step queries_generation_event produced event ReportGenerationEvent\n",
      "Running step generate_report\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m agent = ReportGenerationAgent(\n\u001b[32m      2\u001b[39m     query_engine=query_engine,\n\u001b[32m      3\u001b[39m     llm=llm,\n\u001b[32m      4\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     timeout=\u001b[32m1200.0\u001b[39m,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m report = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(outline=outline)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(report[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreport.md\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:286\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\workflows\\workflow.py:439\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    437\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    443\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\workflows\\context\\context.py:822\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, verbose, run_id, worker_id, resource_manager)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28mself\u001b[39m.write_event_to_stream(\n\u001b[32m    814\u001b[39m     StepStateChanged(\n\u001b[32m    815\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     )\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    823\u001b[39m     kwargs.clear()\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mReportGenerationAgent.generate_report\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    110\u001b[39m outline = ctx.outline\n\u001b[32m    111\u001b[39m queries = ev.queries\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m section_contents = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_section_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m report = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_report(section_contents, outline)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StopEvent(result={\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: report})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mReportGenerationAgent.generate_section_content\u001b[39m\u001b[34m(self, queries, reverse)\u001b[39m\n\u001b[32m     87\u001b[39m             answer = run_llm_sync(\u001b[38;5;28mself\u001b[39m.llm, query + \u001b[33m\"\u001b[39m\u001b[33m Give a short answer.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m             answer = \u001b[38;5;28mstr\u001b[39m(\u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     91\u001b[39m         section_contents[section][subsection] = answer\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m section_contents\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:44\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     43\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m dispatcher.event(\n\u001b[32m     46\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:197\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    194\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    195\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m    196\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_synthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:235\u001b[39m, in \u001b[36mBaseSynthesizer.synthesize\u001b[39m\u001b[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m     query = QueryBundle(query_str=query)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager.event(\n\u001b[32m    232\u001b[39m     CBEventType.SYNTHESIZE,\n\u001b[32m    233\u001b[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001b[32m    234\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     response_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetadataMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     additional_source_nodes = additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    244\u001b[39m     source_nodes = \u001b[38;5;28mlist\u001b[39m(nodes) + \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:43\u001b[39m, in \u001b[36mCompactAndRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_texts = \u001b[38;5;28mself\u001b[39m._make_compact_text_chunks(query_str, text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:179\u001b[39m, in \u001b[36mRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_give_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._refine_response_single(\n\u001b[32m    185\u001b[39m             prev_response, query_str, text_chunk, **response_kwargs\n\u001b[32m    186\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:241\u001b[39m, in \u001b[36mRefine._give_response_single\u001b[39m\u001b[34m(self, query_str, text_chunk, **response_kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    239\u001b[39m         structured_response = cast(\n\u001b[32m    240\u001b[39m             StructuredRefineResponse,\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         query_satisfied = structured_response.query_satisfied\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:85\u001b[39m, in \u001b[36mDefaultRefineProgram.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m     83\u001b[39m         answer = answer.model_dump_json()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer=answer, query_satisfied=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:623\u001b[39m, in \u001b[36mLLM.predict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    622\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:347\u001b[39m, in \u001b[36mOllama.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    344\u001b[39m think = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mthink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.thinking\n\u001b[32m    345\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n\u001b[32m    360\u001b[39m blocks: List[TextBlock | ThinkingBlock] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:351\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    307\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    317\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    318\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    320\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out"
     ]
    }
   ],
   "source": [
    "agent = ReportGenerationAgent(\n",
    "    query_engine=query_engine,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    timeout=1200.0,\n",
    ")\n",
    "\n",
    "report = await agent.run(outline=outline)\n",
    "\n",
    "print(report['response'])\n",
    "\n",
    "with open(\"report.md\", \"w\") as f:\n",
    "    f.write(report['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
