{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd9175c-66cb-4371-8275-a87ab7d265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from instructor import Mode\n",
    "import nest_asyncio\n",
    "from openai import OpenAI\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6ec897-a37a-464d-972a-9834f424f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import urllib.request\n",
    "\n",
    "#ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f266ae-ad40-4656-b1d0-95a67a5d8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM compatibility helpers (add this as a new cell) ---\n",
    "import asyncio\n",
    "from typing import Any\n",
    "\n",
    "def run_llm_sync(llm: Any, prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper that handles:\n",
    "      - Ollama wrapper with .complete()\n",
    "      - HF pipeline-like callables that return list/dict with 'generated_text'\n",
    "      - Generic callable returning a string\n",
    "      - OpenAI client objects\n",
    "    \"\"\"\n",
    "    # NEW: Handle OpenAI client (including instructor-patched ones)\n",
    "    if isinstance(llm, OpenAI):\n",
    "        response = llm.chat.completions.create(\n",
    "            model=\"llama3.1:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_new_tokens,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    # Ollama synchronous wrapper (llama_index.llms.ollama.Ollama provides .complete())\n",
    "    if hasattr(llm, \"complete\") and callable(getattr(llm, \"complete\")):\n",
    "        resp = llm.complete(prompt)\n",
    "        # response may have .text\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "\n",
    "    # If llm is a huggingface pipeline (callable)\n",
    "    if callable(llm):\n",
    "        out = llm(prompt, max_new_tokens=max_new_tokens)\n",
    "        # HF pipeline returns list[{\"generated_text\": \"...\"}]\n",
    "        if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "            return out[0].get(\"generated_text\", \"\").strip()\n",
    "        # Some pipelines return a dict\n",
    "        if isinstance(out, dict) and \"generated_text\" in out:\n",
    "            return out[\"generated_text\"].strip()\n",
    "        # fallback\n",
    "        return str(out).strip()\n",
    "\n",
    "    # Last resort: try calling and stringifying\n",
    "    try:\n",
    "        out = llm(prompt)\n",
    "        return str(out).strip()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM sync call failed: {e}\")\n",
    "\n",
    "async def run_llm_async(llm: Any, prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"\n",
    "    Async wrapper that tries to use llm.acomplete() if present, otherwise\n",
    "    runs the synchronous wrapper in a thread pool.\n",
    "    \"\"\"\n",
    "    # If the wrapper supports async completion (ollama .acomplete)\n",
    "    if hasattr(llm, \"acomplete\") and callable(getattr(llm, \"acomplete\")):\n",
    "        resp = await llm.acomplete(prompt)\n",
    "        if hasattr(resp, \"text\"):\n",
    "            return str(resp.text).strip()\n",
    "        return str(resp).strip()\n",
    "\n",
    "    # Fallback: run the synchronous call in executor\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, run_llm_sync, llm, prompt, max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a93814-5705-4713-8a61-c17d1314353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6fadc-600a-4c9d-8b9c-a210dcee6790",
   "metadata": {},
   "source": [
    "## Download arxiv papers based on topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45ad4c3-9694-43db-b885-ee0fcde1ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_topics = [\"RAG\", \"Agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbda7d8b-63e4-4172-a89c-d2334dc21ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def download_papers(client, topics, num_results_per_topic):\n",
    "    \"\"\"Function to download papers from arxiv for given topics and number of results per topic\"\"\"\n",
    "    for topic in topics:\n",
    "\n",
    "        # sort by recent data and with max results\n",
    "        search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = num_results_per_topic,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        # get the results\n",
    "        results = client.results(search)\n",
    "\n",
    "        # download the pdf\n",
    "        for r in results:\n",
    "            r.download_pdf()\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    # List all .pdf files using pathlib\n",
    "    pdf_files = [file.name for file in Path(directory).glob('*.pdf')]\n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ca0794-2ecc-4255-b1d6-588ea2145c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client\n",
    "client = arxiv.Client()\n",
    "\n",
    "download_papers(client, research_paper_topics, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89c1a8-0892-482d-9bc3-fba9a3f9249e",
   "metadata": {},
   "source": [
    "## Parsing the documents using LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc1f166-7b06-4319-9219-14fa2c4b402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "def parse_files(pdf_files):\n",
    "    \"\"\"Parse PDFs locally (no API key required) and return a flat list of Document objects.\"\"\"\n",
    "    reader = PDFReader()\n",
    "    all_documents = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            docs = reader.load_data(pdf_file)\n",
    "            \n",
    "            # Flatten in case it's a list of tuples or nested lists\n",
    "            if isinstance(docs, tuple):\n",
    "                docs = list(docs)\n",
    "            elif not isinstance(docs, list):\n",
    "                docs = [docs]\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "            print(f\"Parsed: {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse {pdf_file}: {e}\")\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9044a7f9-0e37-4338-abd0-892594735276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 2510.11483v1.Uncertainty_Quantification_for_Retrieval_Augmented_Reasoning.pdf\n",
      "Parsed: 2510.11541v1.Query_Specific_GNN__A_Comprehensive_Graph_Representation_Learning_Method_for_Retrieval_Augmented_Generation.pdf\n",
      "Parsed: 2510.11654v1.FinVet__A_Collaborative_Framework_of_RAG_and_External_Fact_Checking_Agents_for_Financial_Misinformation_Detection.pdf\n",
      "Parsed: 2510.11694v1.Operand_Quant__A_Single_Agent_Architecture_for_Autonomous_Machine_Learning_Engineering.pdf\n",
      "Parsed: 2510.11695v1.When_Agents_Trade__Live_Multi_Market_Trading_Benchmark_for_LLM_Agents.pdf\n",
      "Parsed: 2510.11701v1.Demystifying_Reinforcement_Learning_in_Agentic_Reasoning.pdf\n"
     ]
    }
   ],
   "source": [
    "directory = './'\n",
    "pdf_files = list_pdf_files(directory)\n",
    "\n",
    "documents = parse_files(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56ef05-f1fa-4dea-be6e-21909b7da107",
   "metadata": {},
   "source": [
    "## LlamaIndex Local Logic that substitutes the LlamaCloud Pipeline\n",
    "in an effort to circumvent subscription fees to paid embedding services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab9ee79-55c1-42e4-9f9d-f005a9cd070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 17:56:56,951 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-10-14 17:57:01,326 - INFO - 1 prompt is loaded, with the key: query\n",
      "2025-10-14 17:57:48,440 - INFO - HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Local embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Local chunking configuration\n",
    "transform = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# Flatten parsed LlamaParse outputs into text nodes\n",
    "all_nodes = []\n",
    "for doc in documents:\n",
    "    all_nodes.extend(transform.get_nodes_from_documents([doc]))\n",
    "\n",
    "# Create local vector index\n",
    "index = VectorStoreIndex.from_documents(all_nodes, embed_model=embed_model)\n",
    "\n",
    "# Create a query engine from this index\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ba9e8-ca9d-4a69-804d-4fdd037e8728",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Here, we define some utilities to help us extract metadata from each document.\n",
    "\n",
    "* Metadata - Pydantic model to extract metadata of author names, companies and general AI tags.\n",
    "* get_papers_metadata - Extracts the metadata information from the research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271f390e-5b8c-4a3e-bf63-8618618912c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Output containing the authors names, authors companies, and general AI tags.\"\"\"\n",
    "\n",
    "    author_names: List[str] = Field(..., description=\"List of author names of the paper. Give empty list if not available\")\n",
    "\n",
    "    author_companies: List[str] = Field(..., description=\"List of author companies of the paper. Give empty list if not available\")\n",
    "\n",
    "    ai_tags: List[str] = Field(..., description=\"List of general AI tags related to the paper. Give empty list if not available\")\n",
    "\n",
    "client = instructor.patch(\n",
    "    OpenAI(\n",
    "        base_url='http://localhost:11434/v1',\n",
    "        api_key='ollama',  # required but unused\n",
    "    ),\n",
    "    mode=Mode.JSON,\n",
    ")\n",
    "\n",
    "async def get_papers_metadata(text):\n",
    "    \"\"\"Function to get the metadata from the given paper\"\"\"\n",
    "    prompt = f\"\"\"Generate authors names, authors companies, and general top 3 AI tags for the given research paper.\n",
    "\n",
    "Research Paper:\n",
    "{text}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.1:8b\",\n",
    "        response_model=Metadata,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Then just use client directly for other LLM calls\n",
    "llm = client  # Simple assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df1a4e3-7ce3-4fc0-8611-6cdfae359502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_title(outline):\n",
    "    \"\"\"Function to extract the title from the first line of the outline\"\"\"\n",
    "\n",
    "    first_line = outline.strip().split('\\n')[0]\n",
    "    return first_line.strip('# ').strip()\n",
    "\n",
    "def generate_query_with_llm_local(title, section, subsection, llm):\n",
    "    \"\"\"Generate a concise research query using a local LLM pipeline.\"\"\"\n",
    "    prompt = (\n",
    "        f\"Generate a concise research query for the report titled '{title}'. \"\n",
    "        f\"The query should be for the subsection '{subsection}' under the section '{section}'. \"\n",
    "        f\"The query should help gather relevant information for this part of the report.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Works for local pipelines (e.g. HuggingFace Transformers)\n",
    "        output = llm(prompt, max_new_tokens=100)\n",
    "        \n",
    "        # Handle both dict (transformers) and string outputs\n",
    "        if isinstance(output, list) and \"generated_text\" in output[0]:\n",
    "            return output[0][\"generated_text\"].strip()\n",
    "        elif isinstance(output, dict) and \"generated_text\" in output:\n",
    "            return output[\"generated_text\"].strip()\n",
    "        elif isinstance(output, str):\n",
    "            return output.strip()\n",
    "        else:\n",
    "            return str(output).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate query for {section} → {subsection}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def classify_query(query):\n",
    "    \"\"\"Function to classify the query as either 'LLM' or 'INDEX' based on the query content\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Classify the following query as either \"LLM\" if it can be answered directly by a large language model with general knowledge, or \"INDEX\" if it likely requires querying an external index or database for specific or up-to-date information.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Consider the following:\n",
    "    1. If the query asks for general knowledge, concepts, or explanations, classify as \"LLM\".\n",
    "    2. If the query asks for specific facts, recent events, or detailed information that might not be in the LLM's training data, classify as \"INDEX\".\n",
    "    3. If unsure, err on the side of \"INDEX\".\n",
    "\n",
    "    Classification:\"\"\"\n",
    "\n",
    "    classification = run_llm_sync(llm, prompt).upper()\n",
    "\n",
    "    if classification not in [\"LLM\", \"INDEX\"]:\n",
    "        classification = \"INDEX\"  # Default to INDEX if the response is unclear\n",
    "\n",
    "    return classification\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_outline_and_generate_queries(outline, llm):\n",
    "    \"\"\"\n",
    "    Parse the outline (string or dict) and use a *local* LLM pipeline\n",
    "    to generate one query per subsection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 — Convert outline string → dict automatically\n",
    "    if isinstance(outline, str):\n",
    "        lines = outline.splitlines()\n",
    "        title = \"Untitled Report\"\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip().lstrip('#').strip()\n",
    "            if not line:\n",
    "                continue  # skip blank lines\n",
    "\n",
    "            # Example: \"1. Introduction\"\n",
    "            if re.match(r\"^\\d+\\.\", line) and not re.match(r\"^\\d+\\.\\d+\\.\", line):\n",
    "                current_section = line\n",
    "                sections[current_section] = []\n",
    "\n",
    "            # Example: \"1.1 Background\"\n",
    "            elif re.match(r\"^\\d+\\.\\d+\\.\", line) and current_section:\n",
    "                sections[current_section].append(line)\n",
    "\n",
    "        outline = {\"title\": title, \"sections\": sections}\n",
    "\n",
    "    # Step 2 — Safely access fields\n",
    "    title = outline.get(\"title\", \"Untitled Report\")\n",
    "    sections = outline.get(\"sections\", {})\n",
    "\n",
    "    queries = {}\n",
    "\n",
    "    # Step 3 — Generate a query per subsection using local llm\n",
    "    for section, subsections in sections.items():\n",
    "        queries[section] = {}\n",
    "        for subsection in subsections:\n",
    "            prompt = (\n",
    "                f\"Generate one clear, concise query for the subsection '{subsection}' \"\n",
    "                f\"under section '{section}' in the report titled '{title}'. \"\n",
    "                f\"The query should guide research to gather relevant information.\"\n",
    "            )\n",
    "\n",
    "            # Run the LLM locally (synchronous or pipeline-based)\n",
    "            # Run the LLM using the sync helper function\n",
    "            # 1. Generate the query using the sync helper\n",
    "            query_text = run_llm_sync(llm, prompt)\n",
    "            \n",
    "            # 2. Classify the generated query\n",
    "            classification = classify_query(query_text)\n",
    "            \n",
    "            # 3. Store both the query and its classification in a dictionary\n",
    "            queries[section][subsection] = {\n",
    "                'query': query_text,\n",
    "                'classification': classification\n",
    "            }\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967dc4d-6bc7-4c1b-b542-b2dc981b570d",
   "metadata": {},
   "source": [
    "## ReportGenerationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd8252a-0d0a-40cd-8ff7-c161e4971dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, Context, step\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ReportGenerationEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ReportGenerationAgent(Workflow):\n",
    "    \"\"\"Report generation agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_engine: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.query_engine = query_engine\n",
    "        self.llm = llm or OpenAI(model='gpt-4o-mini')\n",
    "\n",
    "    async def format_report(self, section_contents, outline):\n",
    "        \"\"\"Format the report based on the section contents.\"\"\"\n",
    "        report = \"\"\n",
    "\n",
    "        for section, subsections in section_contents.items():\n",
    "            section_match = re.match(r'^(\\d+\\.)\\s*(.*)$', section)\n",
    "            if section_match:\n",
    "                section_num, section_title = section_match.groups()\n",
    "                \n",
    "                if \"introduction\" in section.lower():\n",
    "                    introduction_num, introduction_title = section_num, section_title\n",
    "                elif \"conclusion\" in section.lower():\n",
    "                    conclusion_num, conclusion_title = section_num, section_title\n",
    "                else:\n",
    "                    combined_content = \"\\n\".join(subsections.values())\n",
    "                    summary_query = f\"Provide a short summary for section '{section}':\\n\\n{combined_content}\"\n",
    "                    section_summary = run_llm_sync(self.llm, summary_query)\n",
    "\n",
    "                    report += f\"# {section_num} {section_title}\\n\\n{section_summary}\\n\\n\"\n",
    "\n",
    "                    report = self.get_subsections_content(subsections, report)\n",
    "\n",
    "        # Add introduction\n",
    "\n",
    "        introduction_query = f\"Create an introduction for the report:\\n\\n{report}\"\n",
    "        introduction = await run_llm_async(self.llm, introduction_query)\n",
    "        report = f\"# {introduction_num} {introduction_title}\\n\\n{introduction}\\n\\n\" + report\n",
    "\n",
    "        # Add conclusion\n",
    "\n",
    "        conclusion_query = f\"Create a conclusion for the report:\\n\\n{report}\"\n",
    "        conclusion = await run_llm_async(self.llm, conclusion_query)\n",
    "        report += f\"# {conclusion_num} {conclusion_title}\\n\\n{conclusion}\"\n",
    "\n",
    "        # Add title\n",
    "        title = extract_title(outline)\n",
    "        report = f\"# {title}\\n\\n{report}\"\n",
    "        return report\n",
    "\n",
    "    def get_subsections_content(self, subsections, report):\n",
    "        \"\"\"Generate content for each subsection in the outline.\"\"\"\n",
    "        # Sort subsections by their keys before adding them to the report\n",
    "        for subsection in sorted(subsections.keys(), key=lambda x: re.search(r'(\\d+\\.\\d+)', x).group(1) if re.search(r'(\\d+\\.\\d+)', x) else x):\n",
    "            content = subsections[subsection]\n",
    "            subsection_match = re.search(r'(\\d+\\.\\d+)\\.\\s*(.+)', subsection)\n",
    "            if subsection_match:\n",
    "                subsection_num, subsection_title = subsection_match.groups()\n",
    "                report += f\"## {subsection_num} {subsection_title}\\n\\n{content}\\n\\n\"\n",
    "            else:\n",
    "                report += f\"## {subsection}\\n\\n{content}\\n\\n\"\n",
    "        return report\n",
    "\n",
    "    def generate_section_content(self, queries, reverse=False):\n",
    "        \"\"\"Generate content for each section and subsection in the outline.\"\"\"\n",
    "        section_contents = {}\n",
    "        for section, subsections in queries.items():\n",
    "            section_contents[section] = {}\n",
    "            subsection_keys = reversed(sorted(subsections.keys())) if reverse else sorted(subsections.keys())\n",
    "            for subsection in subsection_keys:\n",
    "                data = subsections[subsection]\n",
    "                query = data['query']\n",
    "                classification = data['classification']\n",
    "                if classification == \"LLM\":\n",
    "                    # if inside class method and self.llm exists:\n",
    "                    answer = run_llm_sync(self.llm, query + \" Give a short answer.\")\n",
    "\n",
    "                else:\n",
    "                    answer = str(query_engine.query(query))\n",
    "                section_contents[section][subsection] = answer\n",
    "        return section_contents\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def queries_generation_event(self, ctx: Context, ev: StartEvent) -> ReportGenerationEvent:\n",
    "        \"\"\"Generate queries for the report.\"\"\"\n",
    "        \n",
    "        # Store outline directly as an attribute on ctx\n",
    "        ctx.outline = ev.outline\n",
    "    \n",
    "        outline = ctx.outline\n",
    "        queries = parse_outline_and_generate_queries(outline, self.llm)\n",
    "    \n",
    "        return ReportGenerationEvent(queries=queries)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def generate_report(self, ctx: Context, ev: ReportGenerationEvent) -> StopEvent:\n",
    "        \"\"\"Generate report.\"\"\"\n",
    "        \n",
    "        outline = ctx.outline\n",
    "        queries = ev.queries\n",
    "    \n",
    "        section_contents = self.generate_section_content(queries, reverse=True)\n",
    "        report = await self.format_report(section_contents, outline)\n",
    "    \n",
    "        return StopEvent(result={\"response\": report})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca36933c-b76d-456d-995d-5fd2bc339d33",
   "metadata": {},
   "source": [
    "## Outline of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9df5af4-3f29-4400-8ac1-6e0de44988e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = \"\"\"\n",
    "# Research Paper Report on RAG - Retrieval Augmented Generation and Agentic World.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Retrieval Augmented Generation (RAG) and Agents\n",
    "2.1. Fundamentals of RAG and Agents.\n",
    "2.2. Current State and Applications\n",
    "\n",
    "## 3. Latest Papers:\n",
    "3.1. HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings\n",
    "3.2. MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems\n",
    "3.3. VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding\n",
    "\n",
    "## 4. Conclusion:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc625db0-4bca-4292-82ea-8bb7f6c6bd34",
   "metadata": {},
   "source": [
    "## Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b945b5-ac1e-4ee7-a8c3-93727558a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step queries_generation_event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 18:17:21,927 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:19:45,572 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:20:27,829 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:23:22,197 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:24:17,385 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:25:47,007 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:27:38,265 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:29:44,478 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:30:46,709 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-14 18:32:59,297 - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step queries_generation_event produced event ReportGenerationEvent\n",
      "Running step generate_report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 18:33:05,647 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (20.3 GiB) than is available (11.2 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m agent = ReportGenerationAgent(\n\u001b[32m      2\u001b[39m     query_engine=query_engine,\n\u001b[32m      3\u001b[39m     llm=llm,\n\u001b[32m      4\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     timeout=\u001b[32m1200.0\u001b[39m,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m report = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(outline=outline)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(report[\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreport.md\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:286\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\workflows\\workflow.py:439\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    437\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    443\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\workflows\\context\\context.py:822\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, verbose, run_id, worker_id, resource_manager)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28mself\u001b[39m.write_event_to_stream(\n\u001b[32m    814\u001b[39m     StepStateChanged(\n\u001b[32m    815\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     )\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    823\u001b[39m     kwargs.clear()\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mReportGenerationAgent.generate_report\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    110\u001b[39m outline = ctx.outline\n\u001b[32m    111\u001b[39m queries = ev.queries\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m section_contents = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_section_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m report = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_report(section_contents, outline)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StopEvent(result={\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: report})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mReportGenerationAgent.generate_section_content\u001b[39m\u001b[34m(self, queries, reverse)\u001b[39m\n\u001b[32m     87\u001b[39m             answer = run_llm_sync(\u001b[38;5;28mself\u001b[39m.llm, query + \u001b[33m\"\u001b[39m\u001b[33m Give a short answer.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m             answer = \u001b[38;5;28mstr\u001b[39m(\u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     91\u001b[39m         section_contents[section][subsection] = answer\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m section_contents\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:44\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     43\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m dispatcher.event(\n\u001b[32m     46\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:197\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    194\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    195\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m    196\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_response_synthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\base.py:235\u001b[39m, in \u001b[36mBaseSynthesizer.synthesize\u001b[39m\u001b[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m     query = QueryBundle(query_str=query)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager.event(\n\u001b[32m    232\u001b[39m     CBEventType.SYNTHESIZE,\n\u001b[32m    233\u001b[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001b[32m    234\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     response_str = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMetadataMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     additional_source_nodes = additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    244\u001b[39m     source_nodes = \u001b[38;5;28mlist\u001b[39m(nodes) + \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\compact_and_refine.py:43\u001b[39m, in \u001b[36mCompactAndRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[32m     42\u001b[39m new_texts = \u001b[38;5;28mself\u001b[39m._make_compact_text_chunks(query_str, text_chunks)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:179\u001b[39m, in \u001b[36mRefine.get_response\u001b[39m\u001b[34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_give_response_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[32m    184\u001b[39m         response = \u001b[38;5;28mself\u001b[39m._refine_response_single(\n\u001b[32m    185\u001b[39m             prev_response, query_str, text_chunk, **response_kwargs\n\u001b[32m    186\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:241\u001b[39m, in \u001b[36mRefine._give_response_single\u001b[39m\u001b[34m(self, query_str, text_chunk, **response_kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    239\u001b[39m         structured_response = cast(\n\u001b[32m    240\u001b[39m             StructuredRefineResponse,\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m             \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         query_satisfied = structured_response.query_satisfied\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\response_synthesizers\\refine.py:85\u001b[39m, in \u001b[36mDefaultRefineProgram.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m     83\u001b[39m         answer = answer.model_dump_json()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer=answer, query_satisfied=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:623\u001b[39m, in \u001b[36mLLM.predict\u001b[39m\u001b[34m(self, prompt, **prompt_args)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    622\u001b[39m     messages = \u001b[38;5;28mself\u001b[39m._get_messages(prompt, **prompt_args)\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     output = chat_response.message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:347\u001b[39m, in \u001b[36mOllama.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    344\u001b[39m think = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mthink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.thinking\n\u001b[32m    345\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n\u001b[32m    360\u001b[39m blocks: List[TextBlock | ThinkingBlock] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:351\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    307\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    317\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    318\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    320\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documentos\\llama_pipeline\\venv\\Lib\\site-packages\\ollama\\_client.py:133\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (20.3 GiB) than is available (11.2 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "agent = ReportGenerationAgent(\n",
    "    query_engine=query_engine,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    timeout=1200.0,\n",
    ")\n",
    "\n",
    "report = await agent.run(outline=outline)\n",
    "\n",
    "print(report['response'])\n",
    "\n",
    "with open(\"report.md\", \"w\") as f:\n",
    "    f.write(report['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
